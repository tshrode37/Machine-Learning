---
title: "Deep Learning"
author: "Taylor Shrode"
date: "December 13, 2020"
output: word_document
---

# Introduction

Deep Learning performs well in a number of diverse problems due to its ability to provide training stability, generalization, and scalability with big data (Candel & LeDell, 2020). The basic framework of multi-layer neural networks are used to accomplish Deep Learning tasks. This multi-layer, feedforward neural networks framework starts with an input layer that matches the feature space (Candel & LeDell, 2020). This is followed by multiple layers of nonlinearity and ends with a linear regression or classification layer to match the output space. H2O follows the model of multi-layer, feedforward neural networks for predictive modeling (Candel & LeDell, 2020). 

The data we will be using to apply the H2O's Deep Learning algorithm is from the MNIST database of handwritten digits. This data consists of a training set of 60,000 examples and a test set of 10,000 examples (Lecun et al, n.d.). The files available include:

1. Training set images: *train-images-idx3-ubyte.gz*
2. Training set labels: *train-labels-idx1-ubyte.gz*
3. Test set images: *t10k-images-idx3-ubyte.gz*
4. Test set labels: *t10k-labels-idx1-ubyte.gz*

Each image is a standardized $28^2$ pixel greyscale image of
a single handwritten digit (Candel & LeDell, 2020). This data can be loaded into R directly from the website URL and then the files can be unzipped. First, we need to load all the necessary libraries into R. 

## Load Libraries and Data

The packages needed to complete this assignment are loaded below. 

```{r, warning=FALSE, message=FALSE}
library(R.utils)
library(h2o)
library(caret)
```


The **R.utils** package is loaded to unzip our *.gz*  files, or *gunzip* (Bengtsson, 2015). The **h2o** package is loaded to perform Deep Learning. Finally, the **caret** package is loaded to evaluate our Deep Learning models. 

Now, we can load our data into R. To do this, we begin by downloading the data directly from the URL using the **download.file()** function (Dalpiaz, n.d.).

```{r, message = FALSE, warning = FALSE}
download.file("http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz", "train-images-idx3-ubyte.gz")
download.file("http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz", "train-labels-idx1-ubyte.gz")
download.file("http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz", "t10k-images-idx3-ubyte.gz")
download.file("http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz", "t10k-labels-idx1-ubyte.gz")

```

Now, we need to unzip the *.gz* files using the **gunzip()** function (Dalpiaz, n.d.).

```{r}
gunzip("train-images-idx3-ubyte.gz", overwrite = TRUE)
gunzip("train-labels-idx1-ubyte.gz", overwrite = TRUE)
gunzip("t10k-images-idx3-ubyte.gz", overwrite = TRUE)
gunzip("t10k-labels-idx1-ubyte.gz", overwrite = TRUE)
```


To load the image files, we can use the function below. This function has been adapted from Hou (2019). The steps found in the function below are as follows (Hou, 2019):

1. Read Magic Number (A constant numerical or text value used to identify a file format)
2. Read Number of Images
3. Read Number of Rows
4. Read Number of Columns
5. Read pixels of every image, each image has **nrow** x **ncol** pixels
6. Store them in a matrix form for easy visualization  



```{r}
load_image_file <- function(filename) {
  ret = list()
  f = file(filename,'rb')
  readBin(f,'integer',n=1,size=4,endian='big')
  ret$n = readBin(f,'integer',n=1,size=4,endian='big')
  nrow = readBin(f,'integer',n=1,size=4,endian='big')
  ncol = readBin(f,'integer',n=1,size=4,endian='big')
  x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
  ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
  close(f)
  ret
}
```


Now, we can use the function above to load our image files. 

```{r}
trainset <- load_image_file("train-images-idx3-ubyte")
testset <- load_image_file("t10k-images-idx3-ubyte")
```


To load our image labels, we can use the function below, which has been adapted from Hou (2019). The steps found in the function below are as follows (Hou, 2019):

1. Read Magic Number
2. Read Number of Labels
3. Read All the Labels  


```{r}
load_label_file <- function(filename) {
  f = file(filename,'rb')
  readBin(f,'integer',n=1,size=4,endian='big') 
  n = readBin(f,'integer',n=1,size=4,endian='big') 
  y = readBin(f,'integer',n=n,size=1,signed=F) 
  close(f)
  y
}
```

We will now load our labels as a factor, and then add them to our **trainset** and **testset** objects (Dalpiaz, n.d.).

```{r}
trainset_labels <- as.factor(load_label_file("train-labels-idx1-ubyte"))
testset_labels <- as.factor(load_label_file("t10k-labels-idx1-ubyte"))


trainset$labels = trainset_labels
testset$labels  = testset_labels
```

To determine the type of object our data is stored in, we can use the **class()** function (Hou, 2019). 

```{r}
class(trainset)
class(testset)
```


To view the length of each element of a list, we can use the **lengths()** function (Hou, 2019).

```{r, echo=FALSE}
cat("Length of trainset:",lengths(trainset), "\n")
cat("Length of testset:",lengths(testset))
```

Similarly, to determine the dimensions of our data, we can use the commands below (Hou, 2019).

```{r}
dim(as.data.frame(trainset))
dim(as.data.frame(testset))
```


Next, we can define a function to display the images. 

### Display Images

First, we need to define a function to visualize the images in our data (O'Connor, n.d.).  

```{r}
show_digit <- function(arr784, col=gray(12:1/12), ...) {
  image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
```


Using the function above, we can display the fifth label and it's corresponding image/digit in our **trainset** by using the commands below. 

```{r}
trainset$labels[5]
show_digit(trainset$x[5,])
```

According to the the label output, the image below is the number "9". To display the first 25 labels and images, we can use the commands below (Hou, 2019):

```{r}
label_matrix <- t(matrix(trainset$labels[1:25], 5, 5))
label_matrix

par(mfrow=c(5,5))
par(mar=c(0.1,0.1,0.1,0.1))
for(i in 1:25){show_digit(trainset$x[i,])}
```

The output above allows us to easily visualize the images and labels in our **trainset**. Before we continue, we need to reset the **mfrow** parameter. 

```{r}
par(mfrow=c(1,1)) 
```



## Deep Learning Model

As stated above, we will be using the **h2o** package to create our Deep Learning models. To begin, we need to initialize the H2O server.

```{r}
h2o.init(nthreads=-1, enable_assertions  = FALSE)
```

The arguments used above include (Initialize and Connect to H2O, n.d.):

1. *nthreads*: Number of threads in the thread pool. This relates very closely to the number of CPUs used. -1 means use all CPUs on the host (Default).
2. *enable_assertions*: A logical value indicating whether H2O should be launched with assertions enabled. Used mainly for error checking and debugging purposes. This value is only used when R starts H2O.

Next, we need to convert our **trainset** and **testset** lists to H2O objects.


```{r}
h2o.no_progress()
h2o_train <- as.h2o(trainset)
h2o_test <- as.h2o(testset)
```


The first $28^2=784$ values of each row represent the full image and the final values denotes the digit class (label) (Candel & LeDell, 2020). Before we create our Deep Learning model, we need specify the response and predictor columns.


```{r}
y <- "labels"
x <- setdiff(names(h2o_train),y)

tail(x)
```

Above, **y** is our response variable, and **x** contains our predictor columns. Now, we need to encode the response column as categorical for multinomial classification (Candel & LeDell, 2020). 

```{r}
h2o_train[,y] <- as.factor(h2o_train[,y])
h2o_test[,y] <- as.factor(h2o_test[,y])
```

Next, we can train a Deep Learning model and validate on our testing data set. The arguments used in the model below include (Deep Learning, 2020):

1. $x$: Vector containing the names or indices of the predictor variables to use when building the model.
2. $y$: The column to use as the dependent variable.
3. $training\_frame$: The dataset used to build the model.
4. $distribution$: The distribution (i.e., the loss function). The options are AUTO, bernoulli, multinomial, gaussian, poisson, gamma, laplace, quantile, huber, or tweedie. 
    a. Multinomial requires that the response column must be categorical.
5. $activation$: The activation function. Options include: Tanh, Tanh with dropout, Rectifier, Rectifier with dropout, Maxout, Maxout with dropout.
    a. Rectifier and Rectifier with dropout have been known to enhance performance on the MNIST database data (Candel & LeDell, 2020).
6. $hidden$: Specify the hidden layer sizes (e.g., 100,100). The value must be positive. This option defaults to (200,200).
7. $epochs$: Specify the number of times to iterate (stream) the dataset. The value can be a fraction. This option defaults to 10.
8. $input\_dropout\_ratio$: Specify the input layer dropout ratio to improve generalization. Suggested values are 0.1 or 0.2. Defaults to 0.
9. $l1$: Specify the L1 regularization to add stability and improve generalization; sets the value of many weights to 0 (default).
10. $variable\_importance$: Specify whether to compute variable importance.

The H2O Deep Learning model has many parameters, and often it's just the number and sizes of hidden layers, the number of epochs and the activation function and maybe some regularization techniques that need to be changed (Candel, n.d.). 

```{r}
h2o.no_progress()
model <- h2o.deeplearning(x = x,
                          y = y,
                          training_frame  = h2o_train,
                          validation_frame = h2o_test,
                          distribution = "multinomial",
                          activation = "RectifierWithDropout",
                          hidden = c(50,50,50),
                          epochs = 10,
                          input_dropout_ratio = 0.2,
                          l1 = 1e-5,
                          variable_importances = TRUE)
```

Using our model, we can extract the parameters, examine the scoring process and make predictions on new data. 


### View Results of Model


First, we can look at the model summary, which examines the performance of the trained model and displays all performance metrics (Candel & LeDell, 2020).

```{r}
model
```

To view specified parameters of the Deep Learning model, we can use the **model@parameters** command (Candel & LeDell, 2020). The **h2o.performance(model)** function "returns all pre-computed performance metrics for the training/validation set", which can be found in the summary output above under *Training Set Metrics* (Candel & LeDell, 2020). Using the command **h2o.performance(model, valid = TRUE)** returns the validation metrics, which is located under the *Validation Set Metrics* section in the summary output above.

To get the mean square error (MSE), we can use the **h2o.mse()** function. "The MSE metric measures the average of the squares of the errors or deviations. MSE takes the distances from the points to the regression line (these distances are the “errors”) and squaring them to remove any negative signs (Performance and Prediction, n.d.)." 

```{r}
h2o.mse(model, train = TRUE)
h2o.mse(model, valid = TRUE)
```

The output above suggests that our model has an error rate of $\approx 6.1\%$. This then suggests that our model has an accuracy rate of $\approx 93.9\%$. We can confirm this with a confusion matrix, which we will create in the next section. 

To generate the variable importances, we can use the **h2o.varimp()** function. "This feature allows us to view the absolute and relative predictive strength of
each feature in the prediction task (Candel & LeDell, 2020)." Th

```{r}
h2o.varimp(model)
```


The output above shows the top 5 important variables and the least important variables. 

Now, using this model, we can use the **h2o.predict()** function to compute and store predictions on new data (Candel & LeDell, 2020).


### View Predictions

As stated above, we can use the **h2o.predict()** function to make predictions using out model. To view the predictions, we can use the **head()** function. 


```{r}
pred <- h2o.predict(model, newdata = h2o_test)
head(pred)
```

From the output above, we can see the digit that the model predicted for each observation, and the probabilities that the particular observation was a different digit. For example, for the first observation, the model predicted that the image was a "7". The probability that the image is a "7" is 0.9976. The probability that the image is a "1" is 3.3027e-07. 

Now, to view the confusion matrix for our model, we can use the **h2o.confusionMatrix()** function with the model and testing set as arguments (Zalando’s images classification using H2O with R, 2017).

```{r}
h2o.confusionMatrix(model, h2o_test)
```
The output above suggests that the error rate for our model is $\approx 6.75\%$. This confirms our MSE values that was calculated above. Thus, the accuracy rate is `r 100 - 6.75`$\%$.

Another method to creating a confusion matrix is to use the **caret** package and look at the overall accuracy and other statistic values (Charleshsliao, 2017).

```{r}
pred_results <- as.data.frame(pred[,1])
confusionMatrix(unlist(pred_results), testset$labels)$overall
```

Notice, this output also confirms our accuracy rate of $\approx 93.25\%$. This output also provides the Cohen's Kappa Statistic. Recall from the Week 2 Assignment, this value is used to measure the agreement of two raters or methods rating on categorical scales. Kappa values range from 0 to a maximum of 1, which indicates a perfect agreement between the model's predictions and the true values. The value above indicates very good agreement/near perfect agreement. 


## Cartesian Hyper-Parameter Grid Search

For the final portion of this assignment, we will be using a Cartesian grid search for model comparison. "Grid search provides more subtle insights into the model tuning and selection process by inspecting and comparing our trained models after the grid search process is complete (Candell & LeDell, 2020)." To perform a Cartesian grid search, we need to create the hyperparameters. For this, we will create a list of control parameters for the number of hidden layer sizes and the L1 regularization.


```{r}
hidden_opt <- list(c(50,50), c(100,100), c(200,200))
l1_opt <- c(1e-4, 1e-5)
hyper_params <- list(hidden = hidden_opt, l1 = l1_opt)
```

Now, we use the **h2o.grid()** function to launch a grid search. The arguments we will be using below include (H2o. Grid function, n.d.):

1. $algorithm$: Name of algorithm to use in grid search (gbm, randomForest, kmeans, glm, deeplearning, naivebayes, pca).
2. $grid\_id$: ID for resulting grid search.
3. $hyper\_params$: List of lists of hyper parameters
4. $score\_interval$, $stopping\_rounds$, $stopping\_tolerance$, $stopping\_metric$: List of control parameters for smarter hyperparameter search.


```{r}
grid_search <- h2o.grid(algorithm = "deeplearning",
                        grid_id = "mygrid",
                        x = x,
                        y = y,
                        hyper_params = hyper_params,
                        training_frame  = h2o_train,
                        validation_frame = h2o_test,
                        distribution = "multinomial",
                        score_interval = 2,
                        epochs = 10,
                        stopping_rounds = 3,
                        stopping_tolerance = 0.05,
                        stopping_metric = "misclassification")
grid_search
```

Using the function below, we can print out the test MSE for all of the models and their model ID's (Candel & LeDell, 2020).


```{r}
for (model_id in grid_search@model_ids) {
  mse <- h2o.mse(h2o.getModel(model_id), valid = TRUE)
  print(sprintf("Test set MSE: %f", mse))
  print(sprintf("Model ID: %s", model_id))
  }
```

To get the model with the lowest MSE, we can use the commands below because the grid search automatically sorts the models by MSE (Candel, n.d.).


```{r}
best_model <- grid_search@model_ids[[1]]
h2o.mse(h2o.getModel(best_model))
```
Using the "best model", we can make predictions. Similar to above. 

```{r}
pred_search <- h2o.predict(h2o.getModel(best_model), newdata = h2o_test)
head(pred_search)
```
Next, we view the confusion matrix for our model.

```{r}
h2o.confusionMatrix(h2o.getModel(best_model), h2o_test)
```
Notice, our model's error rate went down to $\approx 2.3\%$. To print the accuracy rate, we can use the command below. 


```{r}
pred_search_results <- as.data.frame(pred_search[,1])
confusionMatrix(unlist(pred_search_results), testset$labels)$overall
```


Using the Cartesian grid search method and selecting the "best model", our accuracy increased from $\approx 93.33\%$ to $\approx 97.683\%$. 


# Conclusion

For this assignment, we used H2O's Deep Learning algorithm for classification of handwritten numerals. The data used was from the MNIST database that consisted of of 60,000 training images and 10,000 test images. To load our data, we used the **R.utils** package to "gunzip" our files. For our first Deep Learning model, we mostly used the default settings as our parameters. After computing and storing our predictions made on our testing data, the confusion matrix showed our model had an accuracy rate of $\approx 93\%$. To improve the performance of our model, we performed a Cartesian grid search which allowed us to train several models using combinations of our specified parameters. Choosing the model with the lowest MSE value and using this model to make predictions, the confusion matrix showed that this model had an accuracy rate of $\approx 97\%$. There are many other parameters that could be adjusted and applied. However, without distortions, convolutions, or other advanced image processing techniques, the best-ever published test set error for the MNIST dataset is $0.83\%$
by Microsoft (Candel & LeDell, 2020). This was accomplished after training for 8,000 epochs on ten nodes, which took about ten hours. Typically, the default values will result in good performance for most problems (Candel & LeDell, 2020).



\newpage
# Resources

Bengtsson, H. (2015, February 11). [R] How to unzip a .gz file. https://stat.ethz.ch/pipermail/r-help/2015-February/425709.html

Candel , A., & LeDell, E. (2020). Deep Learning with H2O. http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/2/docs-website/h2o-docs/booklets/DeepLearningBooklet.pdf

Candel, A. (n.d.). Classification and Regression with H2O Deep Learning. GitHub. Retrieved December 13, 2020, from https://github.com/h2oai/h2o-world-2014-training

Charleshsliao. (2017, April 15). A h2o fnn model for mnist. Charles’ Hodgepodge. https://charleshsliao.wordpress.com/2017/04/15/a-h2o-fnn-model-for-mnist/

Dalpiaz, D. (n.d.). Load the MNIST handwritten digits dataset into R as a tidy data frame. Gist. Retrieved December 13, 2020, from https://gist.github.com/daviddalpiaz/ae62ae5ccd0bada4b9acd6dbc9008706

Deep Learning . (2020, November 17). http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.html

H2o. Grid function. (n.d.). Retrieved December 13, 2020, from https://www.rdocumentation.org/packages/h2o/versions/3.32.0.1/topics/h2o.grid

Hou, J. (2019, February 7). Load the mnist dataset. https://rpubs.com/JanpuHou/465274

Initialize and Connect to H2O. (n.d.). Retrieved December 13, 2020, from http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.init.html

Lecun, Y., Cortes, C., & Burges, C. (n.d.). Mnist handwritten digit database. Retrieved December 13, 2020, from http://yann.lecun.com/exdb/mnist/

O’Connor, B. (n.d.). Load the mnist data set in r. Gist. Retrieved December 13, 2020, from https://gist.github.com/brendano/39760

Performance and Prediction. (n.d.). Retrieved December 14, 2020, from http://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html

Zalando’s images classification using H2O with R. (2017, September 12). Appsilon | End­ To­ End Data Science Solutions. https://appsilon.com/zalandos-images-classification-using-h2o-with-r/
